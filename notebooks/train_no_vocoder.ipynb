{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_no_vocoder",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gdLdLbtjto9"
      },
      "source": [
        "This notebook lets you select what data to train on and automatically creates a new folder for the data, trains on it and saves a resynthesized file in the drive\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEx6uejY-w5x"
      },
      "source": [
        "#setup environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_z89CdKIkC9b",
        "cellView": "both"
      },
      "source": [
        "#@title mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "HOME=\"/content/drive/My Drive\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRmB3fUSjozj",
        "cellView": "both"
      },
      "source": [
        "#@title installs\n",
        "%cd /content\n",
        "!rm -rf ddsp_gm2\n",
        "!git clone https://gianmarcohutter/ddsp_gm2\n",
        "%cd ddsp_gm2\n",
        "%tensorflow_version 2.x\n",
        "!pip install -e /content/ddsp_gm2[data_preparation]\n",
        "!pip install -e /content/ddsp_gm2[ddsp] \n",
        "!pip install mir_eval\n",
        "\n",
        "#for phonemes\n",
        "!apt-get update\n",
        "!apt-get install -y swig libpulse-dev\n",
        "!swig -version\n",
        "!pip3 install pocketsphinx\n",
        "!pip3 list | grep pocketsphinx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6PTK5nLUpFB",
        "cellView": "both"
      },
      "source": [
        "#@title imports\n",
        "#dont know if this is needed?\n",
        "import ddsp\n",
        "\n",
        "'''\n",
        "import time\n",
        "\n",
        "from ddsp.training import (data, decoders, encoders, models, preprocessing, \n",
        "                           train_util, trainers)\n",
        "from ddsp.colab.colab_utils import play, specplot, DEFAULT_SAMPLE_RATE\n",
        "import gin\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "'''\n",
        "\n",
        "#for phonemes\n",
        "import itertools\n",
        "import pocketsphinx\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_v0JArPkkWr3",
        "cellView": "both"
      },
      "source": [
        "#@title create test folder\n",
        "#the last one showing up in the list below is where its actually saved\n",
        "%cd {HOME}\n",
        "\n",
        "#make a new testx folder:\n",
        "i=0\n",
        "while(True):\n",
        "    test_folder=\"test\"\n",
        "    i=i+1\n",
        "    test_folder=test_folder+str(i)\n",
        "    print(test_folder)\n",
        "    var=![ -d {test_folder} ] && echo 'does exist'\n",
        "    if(len(var)==0):\n",
        "      !mkdir {test_folder}\n",
        "      #%cd test_folder\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMCLfyX1tCSe",
        "cellView": "both"
      },
      "source": [
        "# copy audio to the test folder\n",
        "DRIVE_DIR=os.path.join(HOME,test_folder)\n",
        "\n",
        "#copy the all songs from that folder into the drive_dir\n",
        "AUDIO_STORAGE=SAVE_DIR = os.path.join(HOME, \"ZHIY_nus/.\")\n",
        "!cp -r \"$AUDIO_STORAGE\" \"$DRIVE_DIR\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6QBVt2Hldcp"
      },
      "source": [
        "!pwd\n",
        "print(DRIVE_DIR)\n",
        "#going back to /content is crucial! otherwise it gets saved in drive and reused the next time the script is run\n",
        "%cd /content/ "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzuU60p2-3K7"
      },
      "source": [
        "#prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yL2g5ZqipENK"
      },
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "AUDIO_DIR = 'data/audio'\n",
        "AUDIO_FILEPATTERN = AUDIO_DIR + '/*'\n",
        "!mkdir -p $AUDIO_DIR\n",
        "\n",
        "if DRIVE_DIR:\n",
        "  SAVE_DIR = os.path.join(DRIVE_DIR, 'GM-Voice')\n",
        "else:\n",
        "  SAVE_DIR = '/content/models/GM-Voice'\n",
        "!mkdir -p \"$SAVE_DIR\"\n",
        "\n",
        "if DRIVE_DIR:\n",
        "  mp3_files = glob.glob(os.path.join(DRIVE_DIR, '*.mp3'))\n",
        "  wav_files = glob.glob(os.path.join(DRIVE_DIR, '*.wav'))\n",
        "  audio_files = mp3_files + wav_files\n",
        "\n",
        "for fname in audio_files:\n",
        "  target_name = os.path.join(AUDIO_DIR, \n",
        "                             os.path.basename(fname).replace(' ', '_'))\n",
        "  print('Copying {} to {}'.format(fname, target_name))\n",
        "  !cp \"$fname\" $target_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOuRbagGuBPs"
      },
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "TRAIN_TFRECORD = 'data/train.tfrecord'\n",
        "TRAIN_TFRECORD_FILEPATTERN = TRAIN_TFRECORD + '*'\n",
        "\n",
        "# Copy dataset from drive if dataset has already been created.\n",
        "drive_data_dir = os.path.join(DRIVE_DIR, 'data') \n",
        "drive_dataset_files = glob.glob(drive_data_dir + '/*')\n",
        "\n",
        "if DRIVE_DIR and len(drive_dataset_files) > 0:\n",
        "  !cp \"$drive_data_dir\"/* data/\n",
        "  print(\"did not prepare the tfrecord new\")\n",
        "\n",
        "else:\n",
        "  # Make a new dataset.\n",
        "  print(\"prepare the tfrecord new\")\n",
        "  if not glob.glob(AUDIO_FILEPATTERN):\n",
        "    raise ValueError('No audio files found. Please use the previous cell to '\n",
        "                    'upload.')\n",
        "\n",
        "  !ddsp_prepare_tfrecord_phonemes \\\n",
        "    --input_audio_filepatterns=$AUDIO_FILEPATTERN \\\n",
        "    --output_tfrecord_path=$TRAIN_TFRECORD \\\n",
        "    --num_shards=10 \\\n",
        "    --alsologtostderr\n",
        "\n",
        "  # Copy dataset to drive for safe-keeping.\n",
        "  if DRIVE_DIR:\n",
        "    !mkdir \"$drive_data_dir\"/\n",
        "    print('Saving to {}'.format(drive_data_dir))\n",
        "    !cp $TRAIN_TFRECORD_FILEPATTERN \"$drive_data_dir\"/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfjgGlj4V9g0"
      },
      "source": [
        "#NEW for Quantile normalization\n",
        "\n",
        "from ddsp.colab import colab_utils\n",
        "import ddsp.training\n",
        "\n",
        "#not sure what provider i need to choose here\n",
        "data_provider = ddsp.training.data.TFRecordProvider(TRAIN_TFRECORD_FILEPATTERN)\n",
        "#data_provider = ddsp.training.data_phoneme.TFRecordProviderPhoneme(TRAIN_TFRECORD_FILEPATTERN)\n",
        "dataset = data_provider.get_dataset(shuffle=False)\n",
        "PICKLE_FILE_PATH = os.path.join(SAVE_DIR, 'dataset_statistics.pkl')\n",
        "\n",
        "colab_utils.save_dataset_statistics(data_provider, PICKLE_FILE_PATH,batch_size=16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "craa8RgyyBZz"
      },
      "source": [
        "from ddsp.colab import colab_utils\n",
        "import ddsp.training\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "data_provider = ddsp.training.data_phoneme.TFRecordProviderPhoneme(TRAIN_TFRECORD_FILEPATTERN)\n",
        "dataset = data_provider.get_dataset(shuffle=False)\n",
        "\n",
        "try:\n",
        "  ex = next(iter(dataset))\n",
        "except StopIteration:\n",
        "  raise ValueError(\n",
        "      'TFRecord contains no examples. Please try re-running the pipeline with '\n",
        "      'different audio file(s).')\n",
        "\n",
        "colab_utils.specplot(ex['audio'])\n",
        "colab_utils.play(ex['audio'])\n",
        "\n",
        "f, ax = plt.subplots(4, 1, figsize=(14, 4))\n",
        "x = np.linspace(0, 4.0, 1000)\n",
        "ax[0].set_ylabel('loudness_db')\n",
        "ax[0].plot(x, ex['loudness_db'])\n",
        "ax[1].set_ylabel('F0_Hz')\n",
        "ax[1].set_xlabel('seconds')\n",
        "ax[1].plot(x, ex['f0_hz'])\n",
        "ax[2].set_ylabel('F0_confidence')\n",
        "ax[2].set_xlabel('seconds')\n",
        "ax[2].plot(x, ex['f0_confidence'])\n",
        "ax[3].set_ylabel('phonemes')\n",
        "ax[3].plot(x,ex['phoneme'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aczVaARj_EnW"
      },
      "source": [
        "#training\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4BKnxjMyDtU"
      },
      "source": [
        "%reload_ext tensorboard\n",
        "import tensorboard as tb\n",
        "tb.notebook.start('--logdir \"{}\"'.format(SAVE_DIR))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCXLJrCcyFW1"
      },
      "source": [
        "  !ddsp_run \\\n",
        "  --mode=train \\\n",
        "  --alsologtostderr \\\n",
        "  --save_dir=\"$SAVE_DIR\" \\\n",
        "  --gin_file=models/ae_spectralloss_more.gin \\\n",
        "  --gin_file=datasets/tfrecord.gin \\\n",
        "  --gin_param=\"TFRecordProvider.file_pattern='$TRAIN_TFRECORD_FILEPATTERN'\" \\\n",
        "  --gin_param=\"batch_size=16\" \\\n",
        "  --gin_param=\"train_util.train.num_steps=30000\" \\\n",
        "  --gin_param=\"train_util.train.steps_per_save=300\" \\\n",
        "  --gin_param=\"trainers.Trainer.checkpoints_to_keep=10\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ia0mBqWIZ2pU"
      },
      "source": [
        "from google.colab import output\n",
        "def beep():\n",
        "  output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')\n",
        "\n",
        "beep()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXMBWPwQyG1k"
      },
      "source": [
        "from ddsp.colab.colab_utils import play, specplot\n",
        "import ddsp.training\n",
        "import gin\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "data_provider = ddsp.training.data_phoneme.TFRecordProviderPhoneme(TRAIN_TFRECORD_FILEPATTERN)\n",
        "dataset = data_provider.get_batch(batch_size=1, shuffle=True)\n",
        "\n",
        "try:\n",
        "  batch = next(iter(dataset))\n",
        "except OutOfRangeError:\n",
        "  raise ValueError(\n",
        "      'TFRecord contains no examples. Please try re-running the pipeline with '\n",
        "      'different audio file(s).')\n",
        "\n",
        "# Parse the gin config.\n",
        "gin_file = os.path.join(SAVE_DIR, 'operative_config-0.gin')\n",
        "gin.parse_config_file(gin_file)\n",
        "\n",
        "# Load model\n",
        "model = ddsp.training.models.Autoencoder()\n",
        "model.restore(SAVE_DIR)\n",
        "\n",
        "# Resynthesize audio.\n",
        "audio_gen = model(batch, training=False)\n",
        "audio = batch['audio']\n",
        "\n",
        "print('Original Audio')\n",
        "specplot(audio)\n",
        "play(audio)\n",
        "\n",
        "print('Resynthesis')\n",
        "specplot(audio_gen)\n",
        "play(audio_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqLM7Dh1yI-C"
      },
      "source": [
        "from ddsp.colab import colab_utils\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "CHECKPOINT_ZIP = 'GM-Voice.zip'\n",
        "latest_checkpoint_fname = os.path.basename(tf.train.latest_checkpoint(SAVE_DIR))\n",
        "!cd \"$SAVE_DIR\" && zip $CHECKPOINT_ZIP $latest_checkpoint_fname* operative_config-0.gin dataset_statistics.pkl\n",
        "!cp \"$SAVE_DIR/$CHECKPOINT_ZIP\" ./\n",
        "#colab_utils.download(CHECKPOINT_ZIP)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAT1SeYR_VHy"
      },
      "source": [
        "#resynthesis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xnzCwaDd5Rg"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "print('Installing from pip package...')\n",
        "#!pip install -qU ddsp\n",
        "!pip install -e /content/ddsp_gm2[ddsp] \n",
        "!pip install mir_eval\n",
        "# Ignore a bunch of deprecation warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import copy\n",
        "import os\n",
        "import time\n",
        "\n",
        "import crepe\n",
        "import ddsp\n",
        "import ddsp.training\n",
        "from ddsp.colab import colab_utils\n",
        "from ddsp.colab.colab_utils import (\n",
        "    auto_tune, detect_notes, fit_quantile_transform, \n",
        "    get_tuning_factor, download, play, record, \n",
        "    specplot, upload, DEFAULT_SAMPLE_RATE)\n",
        "import gin\n",
        "from google.colab import files\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Helper Functions\n",
        "sample_rate = DEFAULT_SAMPLE_RATE  # 16000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlR0t0S3d-A9"
      },
      "source": [
        "#audio_file = \"/content/drive/My Drive/single_sample_deleteme/m3_caro_straight.wav\"\n",
        "#audio_file= \"/content/drive/My Drive/single_sample_deleteme2/elvis_cropped.wav\"\n",
        "audio_file = \"/content/drive/My Drive/eddie_single_short/9_short.wav\"\n",
        "audio,_=librosa.load(audio_file,sr=16000)\n",
        "audio = audio[np.newaxis, :]\n",
        "print('\\nExtracting audio features...')\n",
        "\n",
        "# Plot.\n",
        "specplot(audio)\n",
        "play(audio)\n",
        "\n",
        "# Setup the session.\n",
        "ddsp.spectral_ops.reset_crepe()\n",
        "\n",
        "# Compute features.\n",
        "start_time = time.time()\n",
        "audio_features = ddsp.training.metrics.compute_audio_features(audio)\n",
        "audio_features['loudness_db'] = audio_features['loudness_db'].astype(np.float32)\n",
        "audio_features_mod = None\n",
        "print('Audio features took %.1f seconds' % (time.time() - start_time))\n",
        "\n",
        "\n",
        "TRIM = -15\n",
        "# Plot Features.\n",
        "fig, ax = plt.subplots(nrows=3, \n",
        "                       ncols=1, \n",
        "                       sharex=True,\n",
        "                       figsize=(6, 8))\n",
        "ax[0].plot(audio_features['loudness_db'][:TRIM])\n",
        "ax[0].set_ylabel('loudness_db')\n",
        "\n",
        "ax[1].plot(librosa.hz_to_midi(audio_features['f0_hz'][:TRIM]))\n",
        "ax[1].set_ylabel('f0 [midi]')\n",
        "\n",
        "ax[2].plot(audio_features['f0_confidence'][:TRIM])\n",
        "ax[2].set_ylabel('f0 confidence')\n",
        "_ = ax[2].set_xlabel('Time step [frame]')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukycn-PegW6-"
      },
      "source": [
        "%cd /content\n",
        "#put the path to the drive folder here: \n",
        "#fnames=\"drive/My\\ Drive/test5/ddsp-solo-instrument/my_solo_instrument.zip\"\n",
        "fnames=\"drive/My\\ Drive/\"+test_folder+\"/GM-Voice/GM-Voice.zip\"\n",
        "\n",
        "def find_model_dir(dir_name):\n",
        "  # Iterate through directories until model directory is found\n",
        "  for root, dirs, filenames in os.walk(dir_name):\n",
        "    for filename in filenames:\n",
        "      if filename.endswith(\".gin\") and not filename.startswith(\".\"):\n",
        "        model_dir = root\n",
        "        break\n",
        "  return model_dir \n",
        "\n",
        "# User models.\n",
        "UPLOAD_DIR = '/content/uploaded'\n",
        "!mkdir $UPLOAD_DIR\n",
        "#uploaded_files = files.upload()\n",
        "\n",
        "#!cp $fnames $UPLOAD_DIR\n",
        "print(\"Unzipping... {}\".format(fnames))\n",
        "!unzip -o $fnames -d $UPLOAD_DIR &> /dev/null\n",
        "#!unzip -o \"$UPLOAD_DIR\"\n",
        "\n",
        "\n",
        "model_dir = find_model_dir(UPLOAD_DIR)\n",
        "gin_file = os.path.join(model_dir, 'operative_config-0.gin')\n",
        "\n",
        "\n",
        "# Load the dataset statistics.\n",
        "DATASET_STATS = None\n",
        "dataset_stats_file = os.path.join(model_dir, 'dataset_statistics.pkl')\n",
        "print(f'Loading dataset statistics from {dataset_stats_file}')\n",
        "try:\n",
        "  if tf.io.gfile.exists(dataset_stats_file):\n",
        "    with tf.io.gfile.GFile(dataset_stats_file, 'rb') as f:\n",
        "      DATASET_STATS = pickle.load(f)\n",
        "except Exception as err:\n",
        "  print('Loading dataset statistics from pickle failed: {}.'.format(err))\n",
        "\n",
        "\n",
        "# Parse gin config,\n",
        "with gin.unlock_config():\n",
        "  gin.parse_config_file(gin_file, skip_unknown=True)\n",
        "\n",
        "# Assumes only one checkpoint in the folder, 'ckpt-[iter]`.\n",
        "ckpt_files = [f for f in tf.io.gfile.listdir(model_dir) if 'ckpt' in f]\n",
        "ckpt_name = ckpt_files[0].split('.')[0]\n",
        "ckpt = os.path.join(model_dir, ckpt_name)\n",
        "\n",
        "# Ensure dimensions and sampling rates are equal\n",
        "time_steps_train = gin.query_parameter('DefaultPreprocessor.time_steps')\n",
        "n_samples_train = gin.query_parameter('Additive.n_samples')\n",
        "hop_size = int(n_samples_train / time_steps_train)\n",
        "\n",
        "time_steps = int(audio.shape[1] / hop_size)\n",
        "n_samples = time_steps * hop_size\n",
        "\n",
        "print(\"===Trained model===\")\n",
        "print(\"Time Steps\", time_steps_train)\n",
        "print(\"Samples\", n_samples_train)\n",
        "print(\"Hop Size\", hop_size)\n",
        "print(\"\\n===Resynthesis===\")\n",
        "print(\"Time Steps\", time_steps)\n",
        "print(\"Samples\", n_samples)\n",
        "print('')\n",
        "\n",
        "\n",
        "#take the gin paramters from the zip instead of doing it here?\n",
        "gin_params = [\n",
        "    'RnnFcDecoder.input_keys = (\"ld_scaled\",\"f0_scaled\", \"z\")',\n",
        "    'Additive.n_samples = {}'.format(n_samples),\n",
        "    'FilteredNoise.n_samples = {}'.format(n_samples),\n",
        "    'DefaultPreprocessor.time_steps = {}'.format(time_steps),\n",
        "]\n",
        "\n",
        "with gin.unlock_config():\n",
        "  gin.parse_config(gin_params)\n",
        "\n",
        "\n",
        "# Trim all input vectors to correct lengths \n",
        "for key in ['f0_hz', 'f0_confidence', 'loudness_db']:\n",
        "  audio_features[key] = audio_features[key][:time_steps]\n",
        "audio_features['audio'] = audio_features['audio'][:, :n_samples]\n",
        "\n",
        "\n",
        "# Set up the model just to predict audio given new conditioning\n",
        "model = ddsp.training.models.Autoencoder()\n",
        "model.restore(ckpt)\n",
        "\n",
        "# Build model by running a batch through it.\n",
        "start_time = time.time()\n",
        "_ = model(audio_features, training=False)\n",
        "print('Restoring model took %.1f seconds' % (time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFeKU6O8gaZF"
      },
      "source": [
        "#@title Modify conditioning\n",
        "\n",
        "#@markdown These models were not explicitly trained to perform timbre transfer, so they may sound unnatural if the incoming loudness and frequencies are very different then the training data (which will always be somewhat true). \n",
        "\n",
        "\n",
        "#@markdown ## Note Detection\n",
        "\n",
        "#@markdown You can leave this at 1.0 for most cases\n",
        "threshold = 1 #@param {type:\"slider\", min: 0.0, max:2.0, step:0.01}\n",
        "\n",
        "\n",
        "#@markdown ## Automatic\n",
        "\n",
        "ADJUST = False #@param{type:\"boolean\"}\n",
        "\n",
        "#@markdown Quiet parts without notes detected (dB)\n",
        "quiet = 20 #@param {type:\"slider\", min: 0, max:60, step:1}\n",
        "\n",
        "#@markdown Force pitch to nearest note (amount)\n",
        "autotune = 0 #@param {type:\"slider\", min: 0.0, max:1.0, step:0.1}\n",
        "\n",
        "#@markdown ## Manual\n",
        "\n",
        "\n",
        "#@markdown Shift the pitch (octaves)\n",
        "pitch_shift =  0 #@param {type:\"slider\", min:-2, max:2, step:1}\n",
        "\n",
        "#@markdown Adjsut the overall loudness (dB)\n",
        "loudness_shift = 0 #@param {type:\"slider\", min:-20, max:20, step:1}\n",
        "\n",
        "for k, v in audio_features.items():\n",
        "  print(k)\n",
        "  print(v)\n",
        "audio_features_mod = {k: v.copy() for k, v in audio_features.items()}\n",
        "\n",
        "\n",
        "## Helper functions.\n",
        "def shift_ld(audio_features, ld_shift=0.0):\n",
        "  \"\"\"Shift loudness by a number of ocatves.\"\"\"\n",
        "  audio_features['loudness_db'] += ld_shift\n",
        "  return audio_features\n",
        "\n",
        "\n",
        "def shift_f0(audio_features, pitch_shift=0.0):\n",
        "  \"\"\"Shift f0 by a number of ocatves.\"\"\"\n",
        "  audio_features['f0_hz'] *= 2.0 ** (pitch_shift)\n",
        "  audio_features['f0_hz'] = np.clip(audio_features['f0_hz'], \n",
        "                                    0.0, \n",
        "                                    librosa.midi_to_hz(110.0))\n",
        "  return audio_features\n",
        "\n",
        "\n",
        "mask_on = None\n",
        "\n",
        "if ADJUST and DATASET_STATS is not None:\n",
        "  # Detect sections that are \"on\".\n",
        "  mask_on, note_on_value = detect_notes(audio_features['loudness_db'],\n",
        "                                        audio_features['f0_confidence'],\n",
        "                                        threshold)\n",
        "  \n",
        "  if np.any(mask_on):\n",
        "    # Shift the pitch register.\n",
        "    target_mean_pitch = DATASET_STATS['mean_pitch']\n",
        "    pitch = ddsp.core.hz_to_midi(audio_features['f0_hz'])\n",
        "    mean_pitch = np.mean(pitch[mask_on])\n",
        "    p_diff = target_mean_pitch - mean_pitch\n",
        "    p_diff_octave = p_diff / 12.0\n",
        "    round_fn = np.floor if p_diff_octave > 1.5 else np.ceil\n",
        "    p_diff_octave = round_fn(p_diff_octave)\n",
        "    audio_features_mod = shift_f0(audio_features_mod, p_diff_octave)\n",
        "\n",
        "\n",
        "    # Quantile shift the note_on parts.\n",
        "    _, loudness_norm = colab_utils.fit_quantile_transform(\n",
        "        audio_features['loudness_db'],\n",
        "        mask_on,\n",
        "        inv_quantile=DATASET_STATS['quantile_transform'])\n",
        "\n",
        "    # Turn down the note_off parts.\n",
        "    mask_off = np.logical_not(mask_on)\n",
        "    loudness_norm[mask_off] -=  quiet * (1.0 - note_on_value[mask_off][:, np.newaxis])\n",
        "    loudness_norm = np.reshape(loudness_norm, audio_features['loudness_db'].shape)\n",
        "    \n",
        "    audio_features_mod['loudness_db'] = loudness_norm \n",
        "\n",
        "    # Auto-tune.\n",
        "    if autotune:\n",
        "      f0_midi = np.array(ddsp.core.hz_to_midi(audio_features_mod['f0_hz']))\n",
        "      tuning_factor = get_tuning_factor(f0_midi, audio_features_mod['f0_confidence'], mask_on)\n",
        "      f0_midi_at = auto_tune(f0_midi, tuning_factor, mask_on, amount=autotune)\n",
        "      audio_features_mod['f0_hz'] = ddsp.core.midi_to_hz(f0_midi_at)\n",
        "\n",
        "  else:\n",
        "    print('\\nSkipping auto-adjust (no notes detected or ADJUST box empty).')\n",
        "\n",
        "else:\n",
        "  print('\\nSkipping auto-adujst (box not checked or no dataset statistics found).')\n",
        "\n",
        "# Manual Shifts.\n",
        "audio_features_mod = shift_ld(audio_features_mod, loudness_shift)\n",
        "audio_features_mod = shift_f0(audio_features_mod, pitch_shift)\n",
        "\n",
        "\n",
        "\n",
        "# Plot Features.\n",
        "has_mask = int(mask_on is not None)\n",
        "n_plots = 3 if has_mask else 2 \n",
        "fig, axes = plt.subplots(nrows=n_plots, \n",
        "                      ncols=1, \n",
        "                      sharex=True,\n",
        "                      figsize=(2*n_plots, 8))\n",
        "\n",
        "if has_mask:\n",
        "  ax = axes[0]\n",
        "  ax.plot(np.ones_like(mask_on[:TRIM]) * threshold, 'k:')\n",
        "  ax.plot(note_on_value[:TRIM])\n",
        "  ax.plot(mask_on[:TRIM])\n",
        "  ax.set_ylabel('Note-on Mask')\n",
        "  ax.set_xlabel('Time step [frame]')\n",
        "  ax.legend(['Threshold', 'Likelihood','Mask'])\n",
        "\n",
        "ax = axes[0 + has_mask]\n",
        "ax.plot(audio_features['loudness_db'][:TRIM])\n",
        "ax.plot(audio_features_mod['loudness_db'][:TRIM])\n",
        "ax.set_ylabel('loudness_db')\n",
        "ax.legend(['Original','Adjusted'])\n",
        "\n",
        "ax = axes[1 + has_mask]\n",
        "ax.plot(librosa.hz_to_midi(audio_features['f0_hz'][:TRIM]))\n",
        "ax.plot(librosa.hz_to_midi(audio_features_mod['f0_hz'][:TRIM]))\n",
        "ax.set_ylabel('f0 [midi]')\n",
        "_ = ax.legend(['Original','Adjusted'])\n",
        "\n",
        "print(\"type:\" + str(type(audio_features)))\n",
        "print(audio_features.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWjBpI5jgdRV"
      },
      "source": [
        "#@title #Resynthesize Audio\n",
        "\n",
        "af = audio_features if audio_features_mod is None else audio_features_mod\n",
        "\n",
        "# Run a batch of predictions.\n",
        "start_time = time.time()\n",
        "audio_gen = model(af, training=False)\n",
        "print('Prediction took %.1f seconds' % (time.time() - start_time))\n",
        "\n",
        "# Plot\n",
        "print('Original')\n",
        "play(audio)\n",
        "\n",
        "print('Resynthesis')\n",
        "play(audio_gen)\n",
        "\n",
        "specplot(audio)\n",
        "plt.title(\"Original\")\n",
        "\n",
        "specplot(audio_gen)\n",
        "_ = plt.title(\"Resynthesis\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rl6qEZ89p2SE"
      },
      "source": [
        "print(type(np.array(audio_gen)))\n",
        "\n",
        "librosa.output.write_wav(\"/content/drive/My Drive/\"+test_folder+\"/resynth.wav\",np.array(audio_gen).T,sr=16000)\n",
        "#import scipy\n",
        "#scipy.io.wavfile.write(\"/content/drive/My Drive/\"+test_folder+\"/resynth.wav\",16000,np.array(audio_gen).T)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}