{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "better_harmonic_inharmonic.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "pk319nEyEB6f"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82jpkrvABLMo"
      },
      "source": [
        "notebook that takes harmonic and noise part of the resynthesis and takes the relevant part of each. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pk319nEyEB6f"
      },
      "source": [
        "#getting the signals \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Psep4yGwz1tM"
      },
      "source": [
        "#mount the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "HOME=\"/content/drive/My Drive\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N49GAirAIL2e"
      },
      "source": [
        "%cd /content\n",
        "!rm -rf ddsp_gm2  \n",
        "!git clone https://gianmarcohutter/ddsp_gm2/\n",
        "%cd ddsp_gm2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "6wZde6CBya9k"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "print('Installing from pip package...')\n",
        "#!pip install -qU ddsp\n",
        "!pip install -e /content/ddsp_gm2[ddsp] \n",
        "!pip install mir_eval\n",
        "# Ignore a bunch of deprecation warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import copy\n",
        "import os\n",
        "import time\n",
        "\n",
        "import crepe\n",
        "import ddsp\n",
        "import ddsp.training\n",
        "from ddsp.colab import colab_utils\n",
        "from ddsp.colab.colab_utils import (\n",
        "    auto_tune, detect_notes, fit_quantile_transform, \n",
        "    get_tuning_factor, download, play, record, \n",
        "    specplot, upload, DEFAULT_SAMPLE_RATE)\n",
        "import gin\n",
        "from google.colab import files\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Helper Functions\n",
        "sample_rate = DEFAULT_SAMPLE_RATE  # 16000\n",
        "\n",
        "\n",
        "from google.colab import output\n",
        "def beep():\n",
        "  output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')\n",
        "\n",
        "\n",
        "print('Done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "Go36QW9AS_CD"
      },
      "source": [
        "#@title Record or Upload Audio\n",
        "#@markdown * Either record audio from microphone or upload audio from file (.mp3 or .wav) \n",
        "#@markdown * Audio should be monophonic (single instrument / voice)\n",
        "#@markdown * Extracts fundmanetal frequency (f0) and loudness features. \n",
        "#beep()\n",
        "record_or_upload = \"Upload (.mp3 or .wav)\"  #@param [\"Record\", \"Upload (.mp3 or .wav)\"]\n",
        "\n",
        "record_seconds =     5#@param {type:\"number\", min:1, max:10, step:1}\n",
        "\n",
        "if record_or_upload == \"Record\":\n",
        "  audio = record(seconds=record_seconds)\n",
        "else:\n",
        "  # Load audio sample here (.mp3 or .wav3 file)\n",
        "  # Just use the first file.\n",
        "  filenames, audios = upload()\n",
        "  audio = audios[0]\n",
        "audio = audio[np.newaxis, :]\n",
        "print('\\nExtracting audio features...')\n",
        "\n",
        "# Plot.\n",
        "specplot(audio)\n",
        "play(audio)\n",
        "\n",
        "# Setup the session.\n",
        "ddsp.spectral_ops.reset_crepe()\n",
        "\n",
        "# Compute features.\n",
        "start_time = time.time()\n",
        "audio_features = ddsp.training.metrics.compute_audio_features(audio)\n",
        "audio_features['loudness_db'] = audio_features['loudness_db'].astype(np.float32)\n",
        "audio_features_mod = None\n",
        "print('Audio features took %.1f seconds' % (time.time() - start_time))\n",
        "\n",
        "\n",
        "TRIM = -15\n",
        "# Plot Features.\n",
        "fig, ax = plt.subplots(nrows=3, \n",
        "                       ncols=1, \n",
        "                       sharex=True,\n",
        "                       figsize=(6, 8))\n",
        "ax[0].plot(audio_features['loudness_db'][:TRIM])\n",
        "ax[0].set_ylabel('loudness_db')\n",
        "\n",
        "ax[1].plot(librosa.hz_to_midi(audio_features['f0_hz'][:TRIM]))\n",
        "ax[1].set_ylabel('f0 [midi]')\n",
        "\n",
        "ax[2].plot(audio_features['f0_confidence'][:TRIM])\n",
        "ax[2].set_ylabel('f0 confidence')\n",
        "_ = ax[2].set_xlabel('Time step [frame]')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdqH9wNCFSRY"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GDfqefiLB7A"
      },
      "source": [
        "#going back from ddsp_gm2\n",
        "%cd /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "wmSGDWM5yyjm"
      },
      "source": [
        "\n",
        "#put the path to the drive folder here: \n",
        "#fnames=\"drive/My\\ Drive/test5/ddsp-solo-instrument/my_solo_instrument.zip\"\n",
        "\n",
        "\n",
        "#these three are needed to control what model to take\n",
        "test_folder=\"drive/My\\ Drive/test49/GM-Voice/\"\n",
        "try_intermediate_training = True #choose true if you want to listen to another model than whats in the zip\n",
        "training_steps=32400\n",
        "\n",
        "fnames=test_folder+\"GM-Voice.zip\"\n",
        "intermediate_checkpoint_index=test_folder+\"ckpt-\"+str(training_steps)+\".index\"\n",
        "intermediate_checkpoint_data=test_folder+\"ckpt-\"+str(training_steps)+\".data-00000-of-00001\"\n",
        "statistics=test_folder+\"dataset_statistics.pkl\"\n",
        "config=test_folder+\"operative_config*\"\n",
        "\n",
        "def find_model_dir(dir_name):\n",
        "  # Iterate through directories until model directory is found\n",
        "  for root, dirs, filenames in os.walk(dir_name):\n",
        "    for filename in filenames:\n",
        "      if filename.endswith(\".gin\") and not filename.startswith(\".\"):\n",
        "        model_dir = root\n",
        "        break\n",
        "  return model_dir\n",
        "\n",
        "def find_model_file(dir_name):\n",
        "  # Iterate through directories until model directory is found\n",
        "  for root, dirs, filenames in os.walk(dir_name):\n",
        "    for filename in filenames:\n",
        "      if filename.endswith(\".gin\") and not filename.startswith(\".\"):\n",
        "        model_file = filename\n",
        "        break\n",
        "  return model_file  \n",
        "\n",
        "\n",
        "# User models.\n",
        "UPLOAD_DIR = '/content/uploaded'\n",
        "!rm -r $UPLOAD_DIR\n",
        "!mkdir $UPLOAD_DIR\n",
        "\n",
        "\n",
        "if try_intermediate_training == False:\n",
        "  #just take whats in the .zip\n",
        "  print(\"Unzipping... {}\".format(fnames))\n",
        "  !unzip -o $fnames -d $UPLOAD_DIR &> /dev/null\n",
        "else:\n",
        "  #copy the required files to the upload folder\n",
        "  !cp $intermediate_checkpoint_index $UPLOAD_DIR\n",
        "  !cp $intermediate_checkpoint_data $UPLOAD_DIR\n",
        "  !cp $statistics $UPLOAD_DIR\n",
        "  !cp $config $UPLOAD_DIR\n",
        "\n",
        "model_dir = find_model_dir(UPLOAD_DIR)\n",
        "model_file= find_model_file(UPLOAD_DIR)\n",
        "#gin_file = os.path.join(model_dir, 'operative_config-0.gin')\n",
        "gin_file = os.path.join(model_dir, model_file)\n",
        "\n",
        "# Load the dataset statistics.\n",
        "DATASET_STATS = None\n",
        "dataset_stats_file = os.path.join(model_dir, 'dataset_statistics.pkl')\n",
        "print(f'Loading dataset statistics from {dataset_stats_file}')\n",
        "try:\n",
        "  if tf.io.gfile.exists(dataset_stats_file):\n",
        "    with tf.io.gfile.GFile(dataset_stats_file, 'rb') as f:\n",
        "      DATASET_STATS = pickle.load(f)\n",
        "except Exception as err:\n",
        "  print('Loading dataset statistics from pickle failed: {}.'.format(err))\n",
        "\n",
        "\n",
        "# Parse gin config,\n",
        "with gin.unlock_config():\n",
        "  gin.parse_config_file(gin_file, skip_unknown=True)  \n",
        "\n",
        "# Assumes only one checkpoint in the folder, 'ckpt-[iter]`.\n",
        "ckpt_files = [f for f in tf.io.gfile.listdir(model_dir) if 'ckpt' in f]\n",
        "ckpt_name = ckpt_files[0].split('.')[0]\n",
        "ckpt = os.path.join(model_dir, ckpt_name)\n",
        "\n",
        "# Ensure dimensions and sampling rates are equal\n",
        "time_steps_train = gin.query_parameter('DefaultPreprocessor.time_steps')\n",
        "n_samples_train = gin.query_parameter('Additive.n_samples')\n",
        "hop_size = int(n_samples_train / time_steps_train)\n",
        "\n",
        "time_steps = int(audio.shape[1] / hop_size)\n",
        "n_samples = time_steps * hop_size\n",
        "\n",
        "print(\"===Trained model===\")\n",
        "print(\"Time Steps\", time_steps_train)\n",
        "print(\"Samples\", n_samples_train)\n",
        "print(\"Hop Size\", hop_size)\n",
        "print(\"\\n===Resynthesis===\")\n",
        "print(\"Time Steps\", time_steps)\n",
        "print(\"Samples\", n_samples)\n",
        "print('')\n",
        "\n",
        "\n",
        "# Trim all input vectors to correct lengths \n",
        "for key in ['f0_hz', 'f0_confidence', 'loudness_db']:\n",
        "  audio_features[key] = audio_features[key][:time_steps]\n",
        "audio_features['audio'] = audio_features['audio'][:, :n_samples]\n",
        "\n",
        "\n",
        "gin_params = [\n",
        "    'RnnFcDecoder.input_keys = (\"ld_scaled\", \"f0_scaled\", \"z\")',\n",
        "    #'RnnFcDecoder.input_keys = ( \"ld_scaled\",\"z\")',\n",
        "    'Additive.n_samples = {}'.format(n_samples),\n",
        "    'FilteredNoise.n_samples = {}'.format(n_samples),\n",
        "    'DefaultPreprocessor.time_steps = {}'.format(time_steps),\n",
        "    \"ProcessorGroup.dag = \\\n",
        "    [(@synths.Additive(), ['amps', 'harmonic_distribution', 'f0_hz'])]\"\n",
        "]\n",
        "\n",
        "with gin.unlock_config():\n",
        "  gin.parse_config(gin_params)\n",
        "\n",
        "#GM create two separate models\n",
        "# Set up the model just to predict audio given new conditioning\n",
        "model1 = ddsp.training.models.Autoencoder()\n",
        "model1.restore(ckpt)\n",
        "\n",
        "# Build model by running a batch through it.\n",
        "_ = model1(audio_features, training=False)\n",
        "\n",
        "gin_params2 = [\n",
        "    'RnnFcDecoder.input_keys = (\"ld_scaled\", \"f0_scaled\", \"z\")',\n",
        "    #'RnnFcDecoder.input_keys = (\"ld_scaled\",\"z\")',\n",
        "    'Additive.n_samples = {}'.format(n_samples),\n",
        "    'FilteredNoise.n_samples = {}'.format(n_samples),\n",
        "    'DefaultPreprocessor.time_steps = {}'.format(time_steps),\n",
        "    \"ProcessorGroup.dag = \\\n",
        "    [(@synths.FilteredNoise(), ['noise_magnitudes'])]\"\n",
        "]\n",
        "\n",
        "with gin.unlock_config():\n",
        "  gin.parse_config(gin_params2)\n",
        "\n",
        "model2 = ddsp.training.models.Autoencoder()\n",
        "model2.restore(ckpt)\n",
        "_ = model2(audio_features, training=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "uQFUlIJ_5r36"
      },
      "source": [
        "#@title Modify conditioning\n",
        "\n",
        "#@markdown These models were not explicitly trained to perform timbre transfer, so they may sound unnatural if the incoming loudness and frequencies are very different then the training data (which will always be somewhat true). \n",
        "\n",
        "\n",
        "#@markdown ## Note Detection\n",
        "\n",
        "#@markdown You can leave this at 1.0 for most cases\n",
        "threshold = 1 #@param {type:\"slider\", min: 0.0, max:2.0, step:0.01}\n",
        "\n",
        "\n",
        "#@markdown ## Automatic\n",
        "\n",
        "ADJUST = True #@param{type:\"boolean\"}\n",
        "\n",
        "#@markdown Quiet parts without notes detected (dB)\n",
        "quiet = 20 #@param {type:\"slider\", min: 0, max:60, step:1}\n",
        "\n",
        "#@markdown Force pitch to nearest note (amount)\n",
        "autotune = 0 #@param {type:\"slider\", min: 0.0, max:1.0, step:0.1}\n",
        "\n",
        "#@markdown ## Manual\n",
        "\n",
        "\n",
        "#@markdown Shift the pitch (octaves)\n",
        "pitch_shift =  -1 #@param {type:\"slider\", min:-2, max:2, step:0.1}\n",
        "\n",
        "#@markdown Adjsut the overall loudness (dB)\n",
        "loudness_shift = 20 #@param {type:\"slider\", min:-20, max:20, step:1}\n",
        "\n",
        "for k, v in audio_features.items():\n",
        "  print(k)\n",
        "  print(v)\n",
        "audio_features_mod = {k: v.copy() for k, v in audio_features.items()}\n",
        "\n",
        "\n",
        "## Helper functions.\n",
        "def shift_ld(audio_features, ld_shift=0.0):\n",
        "  \"\"\"Shift loudness by a number of ocatves.\"\"\"\n",
        "  audio_features['loudness_db'] += ld_shift\n",
        "  return audio_features\n",
        "\n",
        "\n",
        "def shift_f0(audio_features, pitch_shift=0.0):\n",
        "  print(pitch_shift)\n",
        "  \"\"\"Shift f0 by a number of ocatves.\"\"\"\n",
        "  audio_features['f0_hz'] *= 2.0 ** (pitch_shift)\n",
        "  audio_features['f0_hz'] = np.clip(audio_features['f0_hz'], \n",
        "                                    0.0, \n",
        "                                    librosa.midi_to_hz(110.0))\n",
        "  return audio_features\n",
        "\n",
        "\n",
        "mask_on = None\n",
        "\n",
        "if ADJUST and DATASET_STATS is not None:\n",
        "  # Detect sections that are \"on\".\n",
        "  mask_on, note_on_value = detect_notes(audio_features['loudness_db'],\n",
        "                                        audio_features['f0_confidence'],\n",
        "                                        threshold)\n",
        "  \n",
        "  if np.any(mask_on):\n",
        "    # Shift the pitch register.\n",
        "    target_mean_pitch = DATASET_STATS['mean_pitch']\n",
        "    pitch = ddsp.core.hz_to_midi(audio_features['f0_hz'])\n",
        "    mean_pitch = np.mean(pitch[mask_on])\n",
        "    p_diff = target_mean_pitch - mean_pitch\n",
        "    p_diff_octave = p_diff / 12.0\n",
        "    round_fn = np.floor if p_diff_octave > 1.5 else np.ceil\n",
        "    p_diff_octave = round_fn(p_diff_octave)\n",
        "    audio_features_mod = shift_f0(audio_features_mod, p_diff_octave)\n",
        "\n",
        "\n",
        "    # Quantile shift the note_on parts.\n",
        "    _, loudness_norm = colab_utils.fit_quantile_transform(\n",
        "        audio_features['loudness_db'],\n",
        "        mask_on,\n",
        "        inv_quantile=DATASET_STATS['quantile_transform'])\n",
        "\n",
        "    # Turn down the note_off parts.\n",
        "    mask_off = np.logical_not(mask_on)\n",
        "    loudness_norm[mask_off] -=  quiet * (1.0 - note_on_value[mask_off][:, np.newaxis])\n",
        "    loudness_norm = np.reshape(loudness_norm, audio_features['loudness_db'].shape)\n",
        "    \n",
        "    audio_features_mod['loudness_db'] = loudness_norm \n",
        "\n",
        "    # Auto-tune.\n",
        "    if autotune:\n",
        "      f0_midi = np.array(ddsp.core.hz_to_midi(audio_features_mod['f0_hz']))\n",
        "      tuning_factor = get_tuning_factor(f0_midi, audio_features_mod['f0_confidence'], mask_on)\n",
        "      f0_midi_at = auto_tune(f0_midi, tuning_factor, mask_on, amount=autotune)\n",
        "      audio_features_mod['f0_hz'] = ddsp.core.midi_to_hz(f0_midi_at)\n",
        "\n",
        "  else:\n",
        "    print('\\nSkipping auto-adjust (no notes detected or ADJUST box empty).')\n",
        "\n",
        "else:\n",
        "  print('\\nSkipping auto-adujst (box not checked or no dataset statistics found).')\n",
        "\n",
        "# Manual Shifts.\n",
        "audio_features_mod = shift_ld(audio_features_mod, loudness_shift)\n",
        "audio_features_mod = shift_f0(audio_features_mod, pitch_shift)\n",
        "\n",
        "\n",
        "\n",
        "# Plot Features.\n",
        "has_mask = int(mask_on is not None)\n",
        "n_plots = 3 if has_mask else 2 \n",
        "fig, axes = plt.subplots(nrows=n_plots, \n",
        "                      ncols=1, \n",
        "                      sharex=True,\n",
        "                      figsize=(2*n_plots, 8))\n",
        "\n",
        "if has_mask:\n",
        "  ax = axes[0]\n",
        "  ax.plot(np.ones_like(mask_on[:TRIM]) * threshold, 'k:')\n",
        "  ax.plot(note_on_value[:TRIM])\n",
        "  ax.plot(mask_on[:TRIM])\n",
        "  ax.set_ylabel('Note-on Mask')\n",
        "  ax.set_xlabel('Time step [frame]')\n",
        "  ax.legend(['Threshold', 'Likelihood','Mask'])\n",
        "\n",
        "ax = axes[0 + has_mask]\n",
        "ax.plot(audio_features['loudness_db'][:TRIM])\n",
        "ax.plot(audio_features_mod['loudness_db'][:TRIM])\n",
        "ax.set_ylabel('loudness_db')\n",
        "ax.legend(['Original','Adjusted'])\n",
        "\n",
        "ax = axes[1 + has_mask]\n",
        "ax.plot(librosa.hz_to_midi(audio_features['f0_hz'][:TRIM]))\n",
        "ax.plot(librosa.hz_to_midi(audio_features_mod['f0_hz'][:TRIM]))\n",
        "ax.set_ylabel('f0 [midi]')\n",
        "_ = ax.legend(['Original','Adjusted'])\n",
        "\n",
        "print(\"type:\" + str(type(audio_features)))\n",
        "print(audio_features.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "SLwg1WkHCXQO"
      },
      "source": [
        "#@title #Resynthesize Audio\n",
        "\n",
        "\n",
        "af = audio_features if audio_features_mod is None else audio_features_mod\n",
        "\n",
        "# Run a batch of predictions.\n",
        "start_time = time.time()\n",
        "\n",
        "harmonic_tensor = model1(af, training=False)\n",
        "noise_tensor = model2(af, training=False)\n",
        "print('Prediction took %.1f seconds' % (time.time() - start_time))\n",
        "\n",
        "# Plot\n",
        "print('Original')\n",
        "play(audio)\n",
        "\n",
        "print('Harmonic Resynthesis')\n",
        "play(harmonic_tensor)\n",
        "\n",
        "print('Noise Resynthesis')\n",
        "play(noise_tensor)\n",
        "\n",
        "specplot(audio)\n",
        "plt.title(\"Original\")\n",
        "\n",
        "specplot(harmonic_tensor)\n",
        "_ = plt.title(\"Harmonic\")\n",
        "\n",
        "specplot(noise_tensor)\n",
        "_ = plt.title(\"Noise\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQX6peghD8uz"
      },
      "source": [
        "#mixing the two signals\n",
        "parameters to change:\n",
        "* CROSSFADE_SAMPLES\n",
        "* minimal_segment_length_noise\n",
        "* minimal_segment_length_harmonic\n",
        "* CONFIDENCE_THRESHOLD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8Yx7OmqENPc"
      },
      "source": [
        "#setting up\n",
        "CONFIDENCE_THRESHOLD= 0.9\n",
        "\n",
        "#get numpy arrays from the tensors\n",
        "harmonic=harmonic_tensor.numpy()[0]\n",
        "noise=noise_tensor.numpy()[0]\n",
        "#get the f0 confidence\n",
        "confidence_short=audio_features['f0_confidence']\n",
        "\n",
        "#spread the confidence array to be equal length as the audio\n",
        "length=len(harmonic)\n",
        "spreading_factor=length/len(confidence_short)\n",
        "print(\"spreading factor: \" + str(spreading_factor))\n",
        "confidence_list=[]\n",
        "for val in confidence_short:\n",
        "  confidence_list+=([val]*int(spreading_factor))\n",
        "confidence=np.array(confidence_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nlx5EgqJlOcs"
      },
      "source": [
        "##Bad example of hard mixing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lMqw-pePzzk"
      },
      "source": [
        "#create a hard mix of the two signals\n",
        "harmonic_confidence=harmonic*(confidence>CONFIDENCE_THRESHOLD)\n",
        "noise_confidence=noise*(confidence<CONFIDENCE_THRESHOLD)\n",
        "mix=harmonic_confidence+noise_confidence\n",
        "\n",
        "#plots\n",
        "fig, ax = plt.subplots(nrows=6,\n",
        "                       ncols=1, \n",
        "                       sharex=True,\n",
        "                       figsize=(30,5))\n",
        "ax[0].plot(harmonic)\n",
        "ax[1].plot(noise)\n",
        "ax[2].plot(confidence)\n",
        "ax[3].plot(harmonic_confidence)\n",
        "ax[4].plot(noise_confidence)\n",
        "ax[5].plot(mix)\n",
        "\n",
        "play(mix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tN_a-x0-laup"
      },
      "source": [
        "##cleaning up the confidence array\n",
        "Only keeping the changes that are followed by a significantly long array of 1 or 0s"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SB2pp3k0kLQ8"
      },
      "source": [
        "def clean_confidence(on_off,threshold_noise,threshold_harmonic):\n",
        "  #args:\n",
        "  #-on_off: numpy array of zeros and ones that has the same lenght as the audio piece\n",
        "  #-threshold: int ,mimal number of consecutive ones or zeros such that it remains\n",
        "  #returns: numpy array where all the short lived changes are cleaned up\n",
        "  threshold=max(threshold_noise,threshold_harmonic)\n",
        "  zero_to_one=[]\n",
        "  one_to_zero=[]\n",
        "  skip_one=False\n",
        "  highest_index=len(on_off)\n",
        "  for i,v in enumerate(on_off):\n",
        "    if(i<highest_index-threshold):\n",
        "      if (v==0 and on_off[i+1]==1):\n",
        "        if skip_one:\n",
        "          skip_one=False\n",
        "        else:\n",
        "          if(check_length(on_off[i+1:i+1+threshold_harmonic],1)):\n",
        "            zero_to_one.append(i)\n",
        "          else:\n",
        "            skip_one=True\n",
        "      elif (v==1 and on_off[i+1]==0):\n",
        "        if skip_one:\n",
        "          skip_one=False\n",
        "        else:\n",
        "          if(check_length(on_off[i+1:i+1+threshold_noise],0)):\n",
        "            one_to_zero.append(i)\n",
        "          else:\n",
        "            skip_one=True\n",
        "  return one_to_zero, zero_to_one\n",
        "\n",
        "\n",
        "def check_length(on_off_snippet,value):\n",
        "  for i in on_off_snippet:\n",
        "    if i!=value:\n",
        "      return False\n",
        "  return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0oztFVEmJ8G"
      },
      "source": [
        "##create a mix of with soft transitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxw073pPbaIT"
      },
      "source": [
        "#create a soft mix with crossfades of the two\n",
        "CROSSFADE_SAMPLES=500\n",
        "\n",
        "watch_window=1000000000 #this is only needed as paramter to change the view of the graphs\n",
        "#minimal_segment_length=int(np.ceil(CROSSFADE_SAMPLES/2)) #creates too many drops\n",
        "minimal_segment_length_noise=10000\n",
        "minimal_segment_length_harmonic=1000\n",
        "\n",
        "\n",
        "#create an array of 1 and zero and see how long the gaps are\n",
        "on_off=[1]*(confidence>CONFIDENCE_THRESHOLD)\n",
        "zero_to_one=[]\n",
        "one_to_zero=[]\n",
        "highest_index=len(on_off)\n",
        "for i,v in enumerate(on_off[:watch_window]):\n",
        "  if(i<highest_index-CROSSFADE_SAMPLES):\n",
        "    if (v==0 and on_off[i+1]==1):\n",
        "      zero_to_one.append(i)\n",
        "    elif (v==1 and on_off[i+1]==0):\n",
        "      one_to_zero.append(i)\n",
        "\n",
        "\n",
        "clean_one_to_zero,clean_zero_to_one=clean_confidence(on_off[:watch_window],minimal_segment_length_noise,minimal_segment_length_harmonic)\n",
        "#transform it into an array that only saves the index of the first in a change\n",
        "#plots\n",
        "fig, ax = plt.subplots(nrows=3,\n",
        "                       ncols=1, \n",
        "                       sharex=True,\n",
        "                       figsize=(30,10))\n",
        "ax[0].plot(on_off[:watch_window])\n",
        "ax[1].vlines(zero_to_one,ymin=0,ymax=1,colors=\"red\")\n",
        "ax[1].vlines(one_to_zero,ymin=0,ymax=1,colors=\"green\")\n",
        "ax[2].vlines(clean_zero_to_one,ymin=0,ymax=1,colors=\"red\")\n",
        "ax[2].vlines(clean_one_to_zero,ymin=0,ymax=1,colors=\"green\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYF3vORgqKes"
      },
      "source": [
        "import math\n",
        "\n",
        "#implement equal power solution from https://dsp.stackexchange.com/questions/14754/equal-power-crossfade\n",
        "def crossFadeEqualVoltage(t):\n",
        "  volumes=[0,0]\n",
        "  volumes[0]=math.sqrt(0.5*(1+t))\n",
        "  volumes[1]=math.sqrt(0.5*(1-t))\n",
        "  return volumes\n",
        "\n",
        "def crossFadeLinear(t):\n",
        "  volumes=[0,0]\n",
        "  volumes[0]=(t+1)/2\n",
        "  volumes[1]=(-t+1)/2\n",
        "  return volumes\n",
        "\n",
        "#using the S curve (a half of a sine) for the fade\n",
        "def crossFadeS(t):\n",
        "  t=t/2+0.5\n",
        "  volumes=[0,0]\n",
        "  volumes[0]=np.sin((np.pi * t)-(np.pi/2))/2 + 0.5\n",
        "  volumes[1]=1-volumes[0]\n",
        "  return volumes\n",
        "\n",
        "#using the S curve (a half of a sine) for the fade\n",
        "#shifting the mixing curves such that the decay starts only when the other signal is fully faded in\n",
        "def crossFadeS2(t):\n",
        "  volumes=[0,0]\n",
        "  if t<0:\n",
        "    t_norm=t+1\n",
        "    volumes[0]=1\n",
        "    volumes[1]=np.sin((np.pi * t)+(np.pi/2))/2 + 0.5\n",
        "  else:\n",
        "    volumes[0]=np.sin((np.pi * t)+(np.pi/2))/2 + 0.5\n",
        "    volumes[1]=1\n",
        "  return volumes\n",
        "\n",
        "def fadeIn(audio,CROSSFADE_SAMPLES):\n",
        "  for t in range(0,CROSSFADE_SAMPLES):\n",
        "    audio[t]=audio[t]*(np.sin((np.pi*t/CROSSFADE_SAMPLES)-(np.pi/2))/2+0.5)\n",
        "  return audio\n",
        "\n",
        "def mix_two(audio1,audio2,cut,cross_length,volumes1,volumes2):\n",
        "  half_cl=int(cross_length/2)\n",
        "  crossing=(audio1[cut-half_cl:cut+half_cl]*volumes1)+(audio2[cut-half_cl:cut+half_cl]*volumes2)\n",
        "  result=np.append(audio1[:cut-half_cl],crossing)\n",
        "  result=np.append(result,audio2[cut+half_cl:])\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G28fpOhB1nYx"
      },
      "source": [
        "volumes1=[]\n",
        "volumes2=[]\n",
        "for i in range(-int(CROSSFADE_SAMPLES/2),int(CROSSFADE_SAMPLES/2)):\n",
        "  t=i/(CROSSFADE_SAMPLES/2)\n",
        "  cr=crossFadeS2(t)\n",
        "  volumes1.append(cr[0])\n",
        "  volumes2.append(cr[1])\n",
        "volumes1=np.array(volumes1)\n",
        "volumes2=np.array(volumes2)\n",
        "plt.plot(volumes1)\n",
        "plt.plot(volumes2)\n",
        "\n",
        "if on_off[0]==0: #track starts with noise\n",
        "  mix=noise\n",
        "  mix=fadeIn(mix,CROSSFADE_SAMPLES)\n",
        "  for i,v in enumerate(clean_zero_to_one):\n",
        "    mix=mix_two(mix,harmonic,v,CROSSFADE_SAMPLES,volumes1,volumes2)\n",
        "    if(i<len(clean_one_to_zero)):\n",
        "      mix=mix_two(mix,noise,clean_one_to_zero[i],CROSSFADE_SAMPLES,volumes1,volumes2)\n",
        "else:\n",
        "  mix=fadeIn(mix,CROSSFADE_SAMPLES)\n",
        "  for i,v in enumerate(clean_one_to_zero):\n",
        "    mix=mix_two(mix,noise,v,CROSSFADE_SAMPLES,volumes1,volumes2)\n",
        "    if(i<len(clean_zero_to_one)):\n",
        "      mix=mix_two(mix,harmonic,clean_zero_to_one[i],CROSSFADE_SAMPLES,volumes1,volumes2)\n",
        "      \n",
        "print(\"harmonic\")\n",
        "play(harmonic)\n",
        "print(len(harmonic))\n",
        "print(\"noise\")\n",
        "play(noise)\n",
        "print('Original Resynthesis')\n",
        "original_resynthesis = harmonic_tensor + noise_tensor\n",
        "play(original_resynthesis)\n",
        "print(\"my mix\")\n",
        "play(mix)\n",
        "print(len(mix))\n",
        "\n",
        " #note: the audible click at the beginning comes from the audio player (is there even if the audio is only 0s)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}